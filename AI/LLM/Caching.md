---
---


https://eugeneyan.com/writing/llm-patterns/#caching-to-reduce-latency-and-cost


https://gptcache.readthedocs.io/en/latest/

https://github.com/docarray/docarray#coming-from-langchain

https://www.youtube.com/live/9VgpXcfJYvw?feature=share&t=1517

#redis 
[llm_memory for ruby](https://github.com/shohey1226/llm_memory)


#laudgpt
* vector database used to cache similar queries and answers. Queries embedded and used as cache look up prior to making request to LLM.

* based on user feedback from each interaction


