---
---


```
hello everyone uh it's a pleasure to be here with you and at least remotely and be able to present this talk on audio quality and its impact on sound and music processing first of all i want to thank the organizers for having invited me to give this talk and be able to present some of the work that we have been doing in my group i am javier sara i am the director of the music technology group at the university at belfabra in barcelona where we work on on many topics all of them related to sound and music computing so in this presentation i will go over some of the work we have been doing on this i will start with an introduction uh with some context and motivation then i will focus on two papers that we have written in the last few years one that is dedicated to the automatic detection of sound artifacts and the other that was dedicated to the impact of audio quality in automatic music classification then i will finish the presentation with some final thoughts so let's start and clearly we first have to agree on what we mean for audio quality but i believe in the context of this workshop uh that's easy uh i think we all are interested in uh in the the audio uh in this case audio on video recordings and in the case of audio on the the possible artifacts that a sound recording a digital sound recording might have and that these artifacts could impact its use in a variety of tasks and a typical task nowadays is uh is applying machine learning techniques to develop models and that use large amounts of audio recordings that we don't have time to go over every single one of them to check the quality that they might have so therefore we are interested in trying to find automatic ways to identify these uh these possible artifacts and to uh try to understand uh what is the impact that they might have in our task to put some context to that i want to present freesound.org freesound.org is a website that in our group mtg we have been maintaining and developing for more than 15 years it's a crowd source framework in the sense that people contribute with sounds and therefore these sounds are of the widest possible variety there is some moderation but even that clearly you can imagine the the wide variety of sounds that this repository has right now we have more than half a million sounds and let's go to uh free sound so this is the website of free sound which i'm just showing um the very recent interface that we have developed still is not fully public so you will be able to see some of the nice design that we have done for it so here you basically search for sounds and then um you uh you can use them for anything you want so let's go through uh some of the the characteristics that this has so in the sounds tab you can get an overview of all the sounds there are and you can see according to different characteristics of the sounds how many are there for each of them so for example here you can see the type of licenses that different sounds have all the licenses are creative common so therefore there are they all allow for users to uh use them in some particular application of course different licenses might have these different restrictions we also have many types of formats the most common one being uncompressed waff files but there is clearly also compressed files like mp3s and some others there is also a wide variety of sampling rates that people upload sounds with and bit depths and bitrates and the number of channels etc etc so if you go to to some of these files for example let's just list all the mp3 files so here you can see them and immediately you can realize the variety of sounds there are so for example clearly we see here one that presumably has been clipped so we can go into that so that even though it might sound okay let's listen to it okay this uh sounds okay but clearly it has uh presumably clip or definitely taken a very the maximum dynamic range and that might create some problems in some situations there might be sounds that are very soft there might be sounds that have different possible recording or background uh noises etc etc anyway so this is uh the reality of um a sound collection and we want to do the best of it of using it in a number of applications so let's uh now go over um the first paper i want to [Music] mention and talk a little bit about which is this paper that we presented in 2019 at the audio engineering society convention which is one of the reference venues for audio engineering work and this paper the title was automatic detection of audio problems for quality control in digital music distribution and this was a collaboration with a company sonosuite that is dedicated to music distribution and let me introduce that concept first so this is the music distribution flow so on a music distribution company uh what does is uh behaves like an interdiment intermediary between the artists or labels and big digital service providers like spotify or or of course amazon music or itunes or you name it so and within that well there's a number of tasks that have to be uh performed but one of them is this audio quality control and that's what sonosuite does and that's clearly a bottleneck because most of it is basically done manually and there is a number of people listening and reviewing the sounds the music and making sure that they're up to the standard that the a particular digital service provider requires so with sono suite we basically develop a number of algorithms which are part of this ascentia library that i will talk about in which well the library covers many topics related to audio processing but for this particular work we develop some specific algorithms for detecting some of these common artifacts or problems that signals have and these here are some of the most of the problems that we identified and that we really want to be able to detect and basically present to the the quality control people so that they can look it over and make sure that uh is adequate for what the the what they need so here are some of the typical problems and like incorrect margins that means too much silence or insufficient silence at the beginning and endings of the song loudness problems that's a very common one that is either too soft or too loud there are always issues related to some possible noises or like humming noises that can can be part of the signal and that are heard and that it can be quite uh quite bad to have there are a lot of audio artifacts like burst doses discontinuities gaps clicks that also some recordings might have there are phase problems that that a signal might have and that in the production process the the phase was not properly handled and also there are problems related with clipping so uh the idea that when a signal is too loud and then it's clipped and that clearly creates some distortion that is not desired so in this uh in this work we develop signal processing algorithms that analyze all of them and we introduced them incorporated into this essentia library so let me just briefly mention about decentia essentia is a c plus plus python library for audio signal processing which has been again developed in our group it has some open license but also it can be uh used commercially and so we offer commercial license for companies that might be interested in um integrating these software into their products in terms of functionality um it has many algorithms uh dedicated to analyzing audio uh so obtaining features that are relevant for many applications and these could be spectrum features features related to rhythm and tempo of a particular piece of music analysis of the tonality and melodic aspects fingerprinting and in the last few years also we have added quite a lot of functionality to support deep learning models in particular tensorflow models both to facilitate the the training but also to facilitate inference in real time so that uh we basically can can can support real time uh machine learning models in essentia and there's a number of design criteria that make it quite useful for tasks like the ones i i am talking about um so it's uh the core of the algorithms are written in c plus so that's quite efficient and it has python wrappers so that is quite easy to to do development and and to integrate these algorithms into different types of environments we have always aimed to to support the use of essentia into large-scale environments and with a lot of large sound collections of both in terms of efficiency and and the way that is uh is developed allows to easy integration into these type of environments as i mentioned it has also uh quite a lot of real-time processing capabilities and finally is cross platform so it's uh different operating systems mobile platforms but also lately also this is javascript so we have uh developed uh the integration of ascentia into the the browser so that's a quite a nice and an efficient way of using essentia so this is the the the website of essentia and let me also go there um so this uh in this um the homepage you can see a lot of uh information about uh the cynthia and links of course to the github repository where you can download and compile or use sent in different ways there is a demos that you can use and to understand some of the things and all the machine learning models but let me just quickly go through um the algorithms so this is the basically all the algorithms that are that are there and let me go to the ones that we are interested in which okay there is fingerprinting and then here okay there is audio problems so this is the list of algorithms that we develop so for example a click detector or detector discontinuities and this idea of the false false stereo with related with the phase issues so if you go to any of these well you have a of course the documentation you have where this comes from where we took the the algorithm from and clearly you can go to uh github and and look at all the implementation details the source code and and be able to integrate it into your own development platform or any any system that you might want to have so anyway so this is uh basically essentia and i encourage you if you are interested for example on the demos page there is quite a few nice demos that uh exemplify some of these things and some of these demos are basically they run on the browser so it's very easy to uh to play around with okay let's continue with the presentation um so the the the valuation we did uh with of these algorithms well we didn't have a ground truth because that's uh that's quite uh demanding to be able to have the annotations of many many sounds so that we could evaluate the quality of our algorithms what we had was uh the many sounds from sonosuite from the company music distribution company that we collaborate with so 300 000 tracks and so what we did was uh to run our algorithms on all of them and basically identify the problems that we found and in here you have the list of all the possible problems that we analyze and the percentage of tracks that had these problems so some of these problems were detected very often so for example insufficient silence at the start or clipping it's like half of the the recordings that we process had some clippings so that's that's huge this humming um i don't think this is uh completely correct so this algorithm detects quite a bit more types of interference beyond the typical humming so we still have to review this and be able to to make it more accurate but anyway these uh we didn't validate it objectively so what we did was uh basically with sonosuite and with the the people that run uh the quality control they they tested and they they saw the the potential of using it together as a sort of semi-automatic type of process that they run this algorithm and then they can focus on uh listening some parts of these sounds so and as a conclusion of this uh work um well we develop a toolkit for audio problem detection it's uh all available in essentia is uh freely available under uh affair gpl for research purposes and also companies can license it and use it so that's that's quite convenient and well we have deployed it in and tested in the in by sono suite they are now in the process of really integrating into their regular workflow and hopefully that will allow them to speed up their quality control process now let's go to the second paper that i wanted to talk about which is this paper that we wrote in 2014 and with the title what is the effect of audio quality on the robustness of mfcc's and chroma features okay so in here it's very much within the work that we are doing on developing essentia audio features and doing a lot of machine learning techniques to to use these features for tasks like gender classification or emotion detection or different analysis of different musical characteristics so and of course our concern was okay how is the audio quality effects these these feature analysis that then is used in these machine learning applications um so we studied different factors try to do it in a very systematic manner so we analyze encoding quality so things like the sampling rate and we focus on two sampling rates 22 050 and 44 100 so to see if there was any impact of uh the sampling rate in the in the in the in the task that we develop then the codec use whether comparing the uncompressed wav file to compress mp3s both a constant bit rate and a variable bit rate mp3s and then we also tried different bit rates for mp3 so going from the lowest quality 64 to very high quality 32 kilobits per second then we analyze the impact on these two features that we talked about the mfcc's and chroma features on two implementations because there's many possible implementations of these uh features so we use the one that we have in essentia and also we compared with the one that it's in this bam plugin from queen mary which is a a quite use implementation so both mfcc's and chroma are uh implemented in those two libraries and then also we we study the impact of the frame size of the the the window size in the in the audio analysis part so from 1024 to huge very large window sizes and also we try to study um the impact of the type of music the genre in uh in this in this in the robustness of these um features okay so we tried uh quite a number of different genres um okay so the two features that we uh focus on one is the the male frequency capsular coefficients no need here now to go into the tail of what it does but this is a very very common audio feature using speech and in music for many tasks as a first step to many applications and um this is a a feature that captures the the timbre characteristics of a sound so it starts from the the the spectrum it it starts from the magnitude spectrum of a signal and then um it has some processing uh that tries to mimic the perceptual characteristics of our hearing system in a way that the the of course the output vector is quite compact but at the same time is uh quite meaningful in terms of uh perceptual validity of that so that's a very very commonly used feature vector for audio and the second feature that we focus on was a chroma type feature one that is called harmonic pitch class profile which is very much used for analyzing the harmonic the tonal aspect of an audio signal so instead of focusing on the timbre here we focus on the let's say the pitch characteristics of uh of a signal and um within chroma there is a many types of implementations and variants and the harmonic pitch class profile is one of them and again it starts from the spectrum but instead of trying to get the overall shape of the spectrum it looks at peaks it looks at the the frequencies and and tries to figure out uh the relationship between these peaks and trying to see if there's harmonic relationships and therefore try to identify the what is called the the pitch class uh of a particular fragment of a sound okay so let's um go more into what we did uh we started from a collection of uh 400 music tracks uh these music tracks included uh many different artists so 395 so basically all were from different artists uh they were uh 10 genres and we only took um 30 seconds uh fragments from these uh from these tracks so it's a quite a short fragment but that's quite uh was quite useful and then we uh focus on uh on two different um versions and that's what we compare no so we compared basically the analysis using a lossless sounds or uncompressed flak file with the different mp3s and different lossy compressions that we identify and that we may know so the whole idea was to compare these uh uh uncompressed the the features uh obtained from this uncompressed sound uh with the features obtained from this compress lossy files and the the process was first we would compute the mfcc's and chroma vectors in all of the files the lossless and the different lossy files and then we would take the the mean of each coefficient so we do we didn't have a frame based type of feature vector so we we basically took the statistics of it and just focus on the mean of each of these coefficients on these 30 seconds fragments and finally what uh we focus on it was to measure the robustness of these coefficients uh on comparing on the original and drive so we we try to find the distance the error between the original and the derived signals about robustness measures uh we took a few so that each one kind of focuses on a different aspect and i'm not going to go into that but each of them has different properties and allows to measure this distance this error between original and compressed file and different uh characteristics of them um clearly the the goal is to to get uh a small error so a high robustness means that there would not be that much difference between the the the coefficients of the original signal and the coefficients uh from the compressed signal so a high robustness therefore low error is good especially if we have different types of encodings and then another aspect that we measure is basically the variance you know the the the stability of these measures and clearly we want to have a small variance so so low stability would be high variance is not good if we have these heterogeneous encodings even though in this case there are ways of trying to control that so this is uh a summary of the the the measures that we obtain for mfcc's so we basically fix mfcc's and then we fix a chroma and then we fix two major sampling rates the 22 000 and the 44 thousand and we fix the two libraries so the library one which is a sentient library two which is the queen mary plugin and then we we basically analyze the these errors these robustness for all um um type of configurations that we had and basically this grand mean is the the robustness uh across all these different possible codecs and variabilities that we took into account and then so grand mean ideally this should be the lowest possible and that would mean a very low error and then the variance is this the stability so again the the lowest uh possible means uh um small variance so that means is high stability and we can compare these again there is no time now to go into the details of everything feel free to um to go over that but some things to be uh notice is for example that this is a robustness every column is robustness and the percentage is the the percentage of the robustness uh coming from one of these features so um clearly there are some features that are responsible for a big part of these uh let's say robustness problems so in this case the bit rate and codec codec is uh is responsible for a big percentage of this robustness problem and also a big percentage of this robustness problem comes from the the track but we don't have much control about that and then these uh this residual this is uh something that we cannot control and so this is not explained by these is quite high that since we cannot control we should not pay much attention on but what is significant is uh the idea that uh there is small differences um between uh the two libraries not much difference between the two sampling rates and between the two libraries uh there is some difference especially if you look at the epsilon which is the euclidean distance you see here that the grand mean is uh much larger in leap one than leap two and the variance is also it's much larger in leap one than in leap two so in that sense there is a much more robustness of leap 2 with respect to the mfcc's implementation okay this can be explained because the implementation is is different and uh in fact the frequency range uh taken by lead to it's much larger it's much smaller than lib1 and therefore lib1 is more sensitive to uh having low bid rates and having less frequency range but anyway uh it's interesting to compare these and uh and there is uh quite a lot to to learn uh from from this in terms of the chroma features the the impact is less well strictly speaking in the the mfcc's the impact is not as large as you could have expected so the different configurations uh do not give that much of a variability and it's pretty robust so mfcc's are quite robust to different codec and bitrates but chroma is even more so even we didn't put them here because they were very very robust so clearly that shows that chroma features being more related to the frequencies not so much the the spectral shape is less impacted by the the different bit rates but again here we can look at the different values and again the general conclusion is that there is not much influence on that again there is uh quite a lot of variability due to the the track and therefore the type of music but that's again something that we uh have not much control of and in fact is interesting and that was also the case on fcc is the frame size even though changing it quite a bit they don't impact uh that much into the result okay and then maybe as a to to to kind of wrap up some of these uh results um what we were especially interested uh was on the on the bitrate and codec and how it would affect so this is would be like a zoom into the different codecs that we use and the robustness factor the the error so the smaller the better and here we can see that um with low bid rates the the error is larger and as the bit rate increases clearly the error gets smaller both for the constant bit rate and the variable bit rate at some point maybe you could say at around 160 192 it reaches a low enough error rate so for clearly low bid rates the error is significant but uh very quickly this error rate uh becomes much much smaller there is some difference between the two libraries so in the sense that lib1 and that's something that i already mentioned only one is is more robust when there is homogeneous encoding but um lip two is more stable with it arteriogenes encoding but again this relates to the implementation of mfcc's and that's something to learn about and and something to consider when you're doing a particular task what implementation or what parameterization of a feature analysis you have to use in order to get the adequate robustness and stability that you need for the collection that you're using so then we tried to do some machine learning on using these features so what we did was to try a simple genre classification tasks using a support vector machine so quite a traditional machine learning model and we run this machine learning model on different sampling rates called the expedit rates and the idea of the features that we use and the libraries that we use and the the the conclusion was sincerely that there was not much of a difference uh i mean this was a very controlled situation and with a decent audio qualities i mean it was not not a very very bad audio into it um also an an interesting conclusion is that of course if you do the training and testing with the same encoding then you gain a much better accuracy and better quality uh that way no but this is something to be continued uh this was a very quick uh kind of test with uh not such a large data set and with a a very traditional machine learning model which should be updated and and we should use some deep learning models that are the ones that are being used nowadays and with that basically i want to to finish and i want to give some final thoughts on on this uh on this topic so the first one is related to the audio quality um clearly we we focus on some particular aspect but audio quality in many sound and music processing applications uh goes much more much beyond this idea of artifacts and coding in fact it's sad but nowadays audio quality is not uh paid much attention to uh in many applications and many services uh people listen on on mobiles on on on devices that are not particularly uh very high quality from an audio quality point of view and people listening in situations that uh clearly they don't they don't sort of pay much attention to the audio quality and there is a lot a lot of things to be considered on audio quality that are very interesting and that can be applied to many tasks beyond the simple ones that i presented here then another important issue and that i of course i touch upon is the idea of data sets no i started presenting free sound which is the source of many data sets that are currently being used in uh in machine learning models and and there are many others of course many sources of data sets and clearly uh there is a a problem of of the variety of audio content that these datasets might have and to assure uh the consistency and the coherence of a data set that's something that typically is not paid much attention to and we should really uh focus on on this uh on making sure that we can control all the different audio quality aspects of a data set and that we take them into a consideration um of course nowadays in machine learning data augmentation strategies pay much attention to these and when we start from a data set the first thing we do is to change coatings modify time stretch frequency shifted so that we can try to emulate a large variety of of sounds beyond what the original data set has and that allows to have a much more robust type of models but still most of these data augmentation don't pay much attention to possible artifacts that these audio quality typically they they just do time stretching or frequency shifting things that are normally done in post-production for for radio programs or for uh some other programs but there is not much on really these type of artifacts that we talked about and tried to uh develop the implementation strategies for that nowadays deep learning is clearly uh the the the main approach for developing machine learning models from audio and for sound and music application originally many of these deep learning models came from image and and therefore were not specific for sound and music nowadays there is a quite a bit of effort and in our research group we are very much into that developing models that take into account sound and music perception and therefore that the models are specifically designed for sound and music computing and that in turn clearly has the implication that things like coding uh and things like uh non-relevant artifacts are are really uh taken into account in in these models and therefore the qualities of the models are improving uh quite a bit compared with the original models that were not based on these type of ideas and finally um well uh as as we have talked about um i believe we can see that there has not been that much work on it in fact preparing for this talk some of these uh the papers i presented are a few years old and i tried to see what had been done since then and i didn't see much so there has not been much work in the world of audio uh i think in the world uh the world of video the there is uh more work but in the work uh the world of audio there has been a little research on the impact of audio quality especially in the current end-to-end models of deep learning in which you start from the audio signal and you basically just do a single model and you obtain the labels at the at the very end and therefore there is no explicit analysis of the audio in the sense of trying to figure out if a possible artifacts are not should not be considered or compression it should be a significant issue so there is clearly a lot of overfitting and a lot of uh problems coming out of this this type of work so i really encourage that there should be more research on this this topic and with that uh i i finished so uh thank you very much for your attention uh it has been very nice to be able to present you this uh this work if you have some question feel free to contact me by email um if you are interested in learning a little bit more about the type of work that we have done and looking at like a free sound looking at the sentient some of the algorithms that we have been developing you can look at our website and i think you will get some interesting further insight so thank you very much bye
```